{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've assembled code/thoughts/steps that may help direct our project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First a line of code that might save us some time -> df.isnull().sum() --> returns null values, by column, for the entire data frame.  value_counts(dropna=False) also includes nan's in the value count, which is helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP: Bivariate Analysis:\n",
    "1) For Continuous Variables: Examine Correlations\n",
    "2) For Categorical Variables: Evaluate distribution of output for each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP: Based on plots generated by Parker we will have to decide which features may benefit from transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP: Prepare the data for analysis:\n",
    "1) Select features to test\n",
    "2) How to handle missing values - eliminate feature vs. dropping missing rows, or both\n",
    "3) Categorical variables -> binary\n",
    "4) Ordinal categorical -> dummify\n",
    "5) Discuss the presence or lack of 5 conditions for linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP: The process for fitting a multivariable linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#General scheme:\n",
    "from sklearn import linear_model\n",
    "ols = linear_model.LinearRegression()\n",
    "X = adver[['TV', 'Radio', 'Newspaper']]  \n",
    "Y = adver['Sales']  # response variable\n",
    "ols.fit(X, Y)\n",
    "print(\"Intercept: %f\" %ols.intercept_)\n",
    "print(\"Coefficients: %s\" %str(ols.coef_))\n",
    "print(\"R^2: %f\" %(ols.score(X, Y)))\n",
    "\n",
    "#Train-Test-Split -- this has been done for us already via test/train data sets, so just informational:\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y_bos, test_size=0.3, random_state=42)\n",
    "print(\"R^2 for train set: %f\" %ols.score(X_train, y_train))\n",
    "print(\"R^2 for test  set: %f\" %ols.score(X_test, y_test))\n",
    "#training error = the error we get applying the model to the same data from which we trained.\n",
    "#test error = the error that we incur on new data.\n",
    "\n",
    "#Because sklearn does not have a nice summary output:\n",
    "import statsmodels.api as sm \n",
    "X_add_const = sm.add_constant(X_train)  #add a column with Beta Zero =1 because X_train does not include Beta Zero\n",
    "#you dont have to do this in sklr, by default it calculates intercept\n",
    "ols = sm.OLS(y_train, X_add_const)\n",
    "ans = ols.fit()\n",
    "print(ans.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP: Test for multi-collinearity by regressing each (continuous) variable against all the others.  Code (lecture) below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_features = ['CRIM', 'INDUS', 'NOX', 'AGE', 'DIS', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
    "scores = {}  #Initialize a list that will hold the scores for each model fit.\n",
    "ols2 = LinearRegression()\n",
    "from sklearn.metrics import r2_score\n",
    "for feature_name in continuous_features:\n",
    "    df2     = df.copy()\n",
    "    feature = df2[feature_name].copy()\n",
    "    df2.drop(feature_name, axis=1, inplace=True)\n",
    "    ols2.fit(df2, feature)\n",
    "    scores[feature_name] = ols2.score(df2, feature)\n",
    "    \n",
    "#The R-Squared of each model is stored in scores; it can be plotted with this code:\n",
    "sns.barplot(x='index', y='R2', data=pd.DataFrame(scores, index=['R2']).T.reset_index())\n",
    "plt.title('$R^2$ of a continuous feature against the other features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_test - lm.predict(birthFeatures)\n",
    "plt.hist(residuals)\n",
    "#------------------\n",
    "print('R^2 is equal to %.3f' % lm.score(birthFeatures, weights)))\n",
    "print('RSS is equal to %.3f' %(np.sum(weights - lm.predict(birthFeatures)) ** 2))  #model explains 57% of the variance\n",
    "print('The intercept is %.3f' % (lm.intercept_)\n",
    "print('The slopes are %s' % str(np.round(lm.coef_, 3))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP: Would it make more sense to predict the log(price) instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.fit(birthFeatures,np.log(weights))\n",
    "residuals = np.log(weights) - lm.predict(birthFeatures)\n",
    "plt.hist(residuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP: Which grouping of features generates the model with the highest R^2?  -- BRUTE FORCE (would be crazy for 100 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pick best 3 predictors, create a model over and over again, record each R^2, pick the best\n",
    "scores = {} # used to record the R^2 of each 3-feature combinations\n",
    "\n",
    "for idx, name1 in enumerate(birthFeatures.columns):\n",
    "        myColumns = birthFeatures.columns[(idx+1):]\n",
    "        for idx2, name2 in enumerate(myColumns):\n",
    "            myColumns2 = myColumns[(idx2+1):]\n",
    "            for idx3, name3 in enumerate(myColumns2):\n",
    "                X2 = birthFeatures[[name1,name2,name3]]\n",
    "                lm.fit(X2, np.log(weights))\n",
    "                scores[(name1,name2,name3)] = lmscore(X2,np.log(weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP: Get rid of features with Low Variance... they don't contribute to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.feature_selection as fs\n",
    "varthres = fs.VarianceThreshold(threshold=1) #initiate object & variance threshold\n",
    "x_new = varthres.fit_transform(iris.data) #calculate variance/remove those features that are below the variance threshold\n",
    "x_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP: Univariate Variable Selection (F-test for Regression / Chi-Sq for Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.chi2(iris.data, iris.target) #returns array of Chi^2 and associated P-values\n",
    "#Select the best features -- k=2 means select the best 2 features\n",
    "best2 = fs.SelectKBest(fs.chi2, k=2).fit_transform(iris.data, iris.target)\n",
    "best2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP: RIDGE REGRESSION -- allows you to analyze data even in the presence of severe multicollinearity.\n",
    "Trades away variance caused by multi-collinearity, but does add some bias\n",
    "LASSO - similar to Ridge but forces coefficients to zero (i.e. eliminates variables/feature selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "ridge = linear_model.Ridge(alpha = 1) # create a ridge regression instance\n",
    "ridge.fit(x, y) # fit data\n",
    "ridge.coef_, ridge.intercept_ # print out the coefficients\n",
    "print(\"The determination of ridge regression is: %.4f\" %ridge.score(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the optimal penalization parameter (Ridge) by looping thru alpha values (lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_100 = np.logspace(0, 8, 100)\n",
    "coef = []\n",
    "for i in alpha_100:\n",
    "    ridge.set_params(alpha = i)\n",
    "    ridge.fit(x, y)\n",
    "    coef.append(ridge.coef_)\n",
    "    \n",
    "df_coef = pd.DataFrame(coef, index=alpha_100, columns=['TV', 'Radio', 'Newspaper'])\n",
    "import matplotlib.pyplot as plt\n",
    "title = 'Ridge coefficients as a function of the regularization'\n",
    "axes = df_coef.plot(logx=True, title=title)\n",
    "axes.set_xlabel('alpha')\n",
    "axes.set_ylabel('coefficients')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
